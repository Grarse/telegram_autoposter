# main.py ‚Äî RSS ‚Üí (—Ñ–∏–ª—å—Ç—Ä —Ç–µ–º) ‚Üí GPT (85% —Ñ–∞–∫—Ç—ã / 15% –∏—Ä–æ–Ω–∏—è) ‚Üí –∫–æ–º–ø–ª–∞–µ–Ω—Å –†–§ ‚Üí Telegram
import os
import re
import json
import time
import queue
import threading
import logging
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from typing import Optional

import feedparser
import httpx
import tldextract
from bs4 import BeautifulSoup
import telebot
from openai import OpenAI

# ----------------- –õ–û–ì–ò -----------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
log = logging.getLogger("autoposter")

# ----------------- ENV -----------------
BOT_TOKEN       = os.getenv("BOT_TOKEN", "").strip()
CHANNEL_ENV     = os.getenv("CHANNEL_ID", "").strip()
OPENAI_API_KEY  = os.getenv("OPENAI_API_KEY", "").strip()
GPT_MODEL       = os.getenv("GPT_MODEL", "gpt-4o-mini").strip()

# –†–µ–∂–∏–º –ø–æ—Å—Ç–æ–≤
POST_MODE       = os.getenv("POST_MODE", "mixed").lower()  # short|long|mixed
LONG_POST_SHARE = int(os.getenv("LONG_POST_SHARE", "20"))  # % –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ (–¥–ª—è mixed)

# –Ø–∑—ã–∫/—Å—Ç–∏–ª—å
POST_LANG       = os.getenv("POST_LANG", "ru").lower()
FACT_RATIO      = float(os.getenv("FACT_RATIO", "0.85"))   # 85% —Ñ–∞–∫—Ç—ã / 15% –∏—Ä–æ–Ω–∏—è
SARCASM_LEVEL   = os.getenv("SARCASM_LEVEL", "medium")     # none|low|medium|high

# –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã
POST_INTERVAL_MIN = int(os.getenv("POST_INTERVAL_MIN", "30"))  # –ø–ª–∞–Ω–æ–≤—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
RSS_POLL_SEC      = int(os.getenv("RSS_POLL_SEC", "120"))      # —á–∞—Å—Ç–æ—Ç–∞ –æ–ø—Ä–æ—Å–∞ RSS

# –î–ª–∏–Ω—ã
SHORT_MIN = int(os.getenv("SHORT_MIN_CHARS", "400"))
SHORT_MAX = int(os.getenv("SHORT_MAX_CHARS", "600"))
LONG_MIN  = int(os.getenv("LONG_MIN_CHARS",  "1000"))
LONG_MAX  = int(os.getenv("LONG_MAX_CHARS",  "1200"))

# Breaking
ALLOW_BREAKING = os.getenv("ALLOW_BREAKING", "true").lower() == "true"
_breaking_raw  = os.getenv("BREAKING_KEYWORDS_RU", "—Å—Ä–æ—á–Ω–æ;–º–æ–ª–Ω–∏—è;breaking;urgent")
BREAKING_WORDS = [w.strip().lower() for w in re.split(r"[;,]", _breaking_raw) if w.strip()]

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏/—Ç–µ–º—ã/—Ä—É–±—Ä–∏–∫–∏
FEEDS_JSON       = os.getenv("FEEDS_JSON", "[]")
TOPIC_FILTERS    = json.loads(os.getenv("TOPIC_FILTERS_JSON", "[]"))  # —Å–ø–∏—Å–æ–∫ regex
RUBRICS          = json.loads(os.getenv("RUBRICS_JSON", "{}"))        # {regex: {emoji,title}}

# –ë–ª–æ–∫–∏ –ø–æ—Å—Ç–∞
SOURCE_BLOCK     = os.getenv("SOURCE_BLOCK", "on").lower() == "on"
ALLOW_IMAGES     = os.getenv("ALLOW_IMAGES", "true").lower() == "true"

# –ö–æ–º–ø–ª–∞–µ–Ω—Å –†–§ (–∏–Ω–æ–∞–≥–µ–Ω—Ç—ã/–∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ)
LEGAL_ENABLED        = os.getenv("LEGAL_FOOTNOTES", "on").lower() == "on"
LEGAL_REFRESH_HOURS  = int(os.getenv("LEGAL_REFRESH_HOURS", "24"))
EXTRA_ALIASES_ENV    = os.getenv("COMPLIANCE_EXTRA_ALIASES", "").strip()
try:
    EXTRA_ALIASES = json.loads(EXTRA_ALIASES_ENV) if EXTRA_ALIASES_ENV else []
except Exception:
    EXTRA_ALIASES = []

# –í–∞–ª–∏–¥–∞—Ü–∏—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö ENV
if not BOT_TOKEN or not CHANNEL_ENV or not OPENAI_API_KEY:
    raise SystemExit("ENV –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã: BOT_TOKEN, CHANNEL_ID, OPENAI_API_KEY")

try:
    CHANNEL_ID: int | str = int(CHANNEL_ENV) if CHANNEL_ENV.startswith("-100") else CHANNEL_ENV
except Exception:
    CHANNEL_ID = CHANNEL_ENV  # –ø–æ–∑–≤–æ–ª–∏–º @username

# ----------------- –ö–õ–ò–ï–ù–¢–´ -----------------
bot = telebot.TeleBot(BOT_TOKEN, parse_mode="HTML")

# –û—Ç–∫–ª—é—á–∞–µ–º –ª—é–±—ã–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è (—Ñ–∏–∫—Å –ø—Ä–æ–±–ª–µ–º —Å 'proxies' –≤ SDK OpenAI)
for k in ["HTTP_PROXY","HTTPS_PROXY","ALL_PROXY","http_proxy","https_proxy","all_proxy","OPENAI_PROXY"]:
    os.environ.pop(k, None)

http_client = httpx.Client(trust_env=False, timeout=30.0)
oai = OpenAI(api_key=OPENAI_API_KEY, http_client=http_client)

# ----------------- –ú–û–î–ï–õ–ò -----------------
@dataclass
class NewsItem:
    title: str
    summary: str
    link: str
    published: Optional[datetime]
    breaking: bool

# ----------------- –û–ß–ï–†–ï–î–¨/–î–ï–î–£–ü -----------------
seen_ids: set[str] = set()
news_q: "queue.Queue[NewsItem]" = queue.Queue()

# ----------------- –£–¢–ò–õ–ò–¢–´ -----------------
def strip_html(s: str) -> str:
    if not s:
        return ""
    # –ë—ã—Å—Ç—Ä–æ —É–±–µ—Ä—ë–º —Ç–µ–≥–∏
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def is_breaking(title: str, summary: str) -> bool:
    t = f"{title} {summary}".lower()
    return any(w and w in t for w in BREAKING_WORDS)

def allowed_topic(title: str, summary: str) -> bool:
    if not TOPIC_FILTERS:
        return True
    text = f"{title}\n{summary}".lower()
    return any(re.search(rgx, text) for rgx in TOPIC_FILTERS)

def load_feeds() -> list[str]:
    try:
        arr = json.loads(FEEDS_JSON)
        return [x for x in arr if isinstance(x, str)]
    except Exception:
        return []

def domain_of(url: str) -> str:
    try:
        return tldextract.extract(url).registered_domain or ""
    except Exception:
        return ""

def pick_rubric(title: str, summary: str, link: str) -> tuple[str, str]:
    text = f"{title}\n{summary}\n{link}".lower()
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
    for regex, cfg in RUBRICS.items():
        try:
            if re.search(regex, text):
                return cfg.get("emoji", ""), cfg.get("title", "")
        except re.error:
            continue
    # Fallback –ø–æ –¥–æ–º–µ–Ω—É/–∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    dom = domain_of(link)
    if "coin" in dom or "crypto" in dom:
        return "ü™ô", "–ö—Ä–∏–ø—Ç–æ"
    if re.search(r"\b(ai|–∏—Å–∫—É—Å—Å—Ç–≤.*–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç|–Ω–µ–π—Ä–æ—Å–µ—Ç|openai|nvidia)\b", text):
        return "ü§ñ", "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ AI"
    return "üìà", "–†—ã–Ω–∫–∏ –∏ —ç–∫–æ–Ω–æ–º–∏–∫–∞"

def pick_length() -> tuple[int, int, bool]:
    # —É—á–∏—Ç—ã–≤–∞–µ–º POST_MODE –∏ LONG_POST_SHARE
    if POST_MODE == "short":
        return (SHORT_MIN, SHORT_MAX, False)
    if POST_MODE == "long":
        return (LONG_MIN, LONG_MAX, True)
    # mixed
    import random
    if random.randint(1, 100) <= LONG_POST_SHARE:
        return (LONG_MIN, LONG_MAX, True)
    return (SHORT_MIN, SHORT_MAX, False)

def sarcasm_hint() -> str:
    return {
        "high":   "–î–æ–±–∞–≤—å –∑–∞–º–µ—Ç–Ω—ã–π, –Ω–æ —É–º–µ—Å—Ç–Ω—ã–π —Å–∞—Ä–∫–∞–∑–º (–Ω–µ –ø–µ—Ä–µ—Ö–æ–¥–∏ –Ω–∞ –ª–∏—á–Ω–æ—Å—Ç–∏).",
        "medium": "–î–æ–±–∞–≤—å –ª—ë–≥–∫—É—é –∏—Ä–æ–Ω–∏—é/—Å–∞—Ä–∫–∞–∑–º (–æ—á–µ–Ω—å —É–º–µ—Ä–µ–Ω–Ω–æ).",
        "low":    "–î–æ–±–∞–≤—å –µ–¥–≤–∞ –∑–∞–º–µ—Ç–Ω—É—é –∏—Ä–æ–Ω–∏—é.",
        "none":   "–ë–µ–∑ —Å–∞—Ä–∫–∞–∑–º–∞.",
    }.get(SARCASM_LEVEL, "–î–æ–±–∞–≤—å –ª—ë–≥–∫—É—é –∏—Ä–æ–Ω–∏—é/—Å–∞—Ä–∫–∞–∑–º (–æ—á–µ–Ω—å —É–º–µ—Ä–µ–Ω–Ω–æ).")

# ----------------- GPT -----------------
def build_prompt(title: str, summary: str, link: str, rubric: str, lo: int, hi: int) -> str:
    return f"""
–¢—ã ‚Äî —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–≥–æ —Ç–µ–ª–µ–≥—Ä–∞–º-–∫–∞–Ω–∞–ª–∞.
–°—Ç–∏–ª—å: {int(FACT_RATIO*100)}% —Ñ–∞–∫—Ç—É—Ä–∞ / {100-int(FACT_RATIO*100)}% –∏—Ä–æ–Ω–∏—è; –∫–æ—Ä–æ—Ç–∫–∏–µ –∞–±–∑–∞—Ü—ã; –±–µ–∑ —Ö—ç—à—Ç–µ–≥–æ–≤.
–û–ø–∏—Ä–∞–π—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (title/summary/link). –ù–∏–∫–∞–∫–æ–π –≤—ã–¥—É–º–∫–∏.

–í—ã–≤–µ–¥–∏ –°–¢–†–û–ì–û –ø–æ —Å–µ–∫—Ü–∏—è–º:

1) –õ–∏–¥ ‚Äî 1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø–æ —Å—É—Ç–∏.
2) –§–∞–∫—Ç—ã ‚Äî 2‚Äì3 –º–∞—Ä–∫–µ—Ä–∞ —Å —Ü–∏—Ñ—Ä–∞–º–∏/–∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–æ–π –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –∏–∑ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
3) –ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç? ‚Äî 1‚Äì2 –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –≤—ã–≤–æ–¥–∞ –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è/–∏–Ω–≤–µ—Å—Ç–æ—Ä–∞.
4) –û—Å—Ç—Ä–æ—É–º–Ω–∞—è —Ä–µ–ø–ª–∏–∫–∞ ‚Äî 1 –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ç—Ä–æ–∫–∞, —É–º–µ—Å—Ç–Ω–∞—è –∏ –Ω–µ —Ç–æ–∫—Å–∏—á–Ω–∞—è.

–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:
- –ï—Å–ª–∏ —Ü–∏—Ñ—Ä –Ω–µ—Ç ‚Äî —É–∫–∞–∂–∏ –∫–ª—é—á–µ–≤—ã–µ –¥—Ä–∞–π–≤–µ—Ä—ã/—Ä–∏—Å–∫–∏ –≤–º–µ—Å—Ç–æ —Ü–∏—Ñ—Ä.
- –£–º–µ—Ä–µ–Ω–Ω–∞—è –∏—Ä–æ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä–Ω–æ 15% —Ç–æ–Ω–∞). {sarcasm_hint()}
- –ë–µ–∑ –ø—Ä–∏–∑—ã–≤–æ–≤ –ø–æ–¥–ø–∏—Å—ã–≤–∞—Ç—å—Å—è, –±–µ–∑ —Ö—ç—à—Ç–µ–≥–æ–≤.

–†—É–±—Ä–∏–∫–∞: {rubric}
–¢—Ä–µ–±—É–µ–º—ã–π –æ–±—ä—ë–º: {lo}-{hi} —Å–∏–º–≤–æ–ª–æ–≤.
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}
–ö—Ä–∞—Ç–∫–æ: {summary}
–°—Å—ã–ª–∫–∞: {link}
"""

def gpt_generate_post(title: str, summary: str, link: str, emoji: str, rubric: str) -> str:
    lo, hi, is_long = pick_length()
    prompt = build_prompt(title, summary, link, rubric, lo, hi)
    resp = oai.chat.completions.create(
        model=GPT_MODEL,
        messages=[
            {"role": "system", "content": "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, —è—Å–Ω–æ –∏ —Å–¥–µ—Ä–∂–∞–Ω–Ω–æ."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.6, top_p=0.9, max_tokens=800
    )
    body = (resp.choices[0].message.content or "").strip()
    header = f"{emoji} <b>{rubric}</b>\n" if rubric else ""
    return header + body

# ----------------- –ö–û–ú–ü–õ–ê–ï–ù–° –†–§ -----------------
URL_MINJUST = "https://minjust.gov.ru/ru/documents/7822/"
URL_FSB     = "http://www.fsb.ru/fsb/npd/terror.htm"

# –ë–∞–∑–æ–≤—ã–µ –∞–ª–∏–∞—Å—ã
BASE_ALIASES = [
    {"canonical": "Meta", "aliases": ["Meta", "Facebook", "Instagram", "WhatsApp", "Messenger", "Threads"]},
]

def _norm(s: str) -> str:
    s = s.lower().strip()
    s = s.replace("—ë", "–µ")
    s = re.sub(r"[¬´¬ª‚Äú‚Äù\"'`‚Äô]", "", s)
    s = re.sub(r"\s+", " ", s)
    return s

_legal_cache = {"ts": None, "agents": set(), "banned": set(), "alias_map": {}}

def _build_alias_map(canonicals: set) -> dict:
    amap = {}
    # –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ —Ç–æ–∂–µ –∞–ª–∏–∞—Å—ã
    for c in canonicals:
        n = _norm(c)
        if n:
            amap[n] = c
    # –±–∞–∑–æ–≤—ã–µ –∞–ª–∏–∞—Å—ã
    for item in BASE_ALIASES:
        canon = item.get("canonical", "").strip()
        if not canon: continue
        amap[_norm(canon)] = canon
        for a in item.get("aliases", []):
            amap[_norm(a)] = canon
    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    for item in EXTRA_ALIASES:
        canon = item.get("canonical", "").strip()
        if not canon: continue
        amap[_norm(canon)] = canon
        for a in item.get("aliases", []):
            amap[_norm(a)] = canon
    return amap

def _http_get_text(url: str) -> str:
    r = http_client.get(url)
    r.raise_for_status()
    return r.text

def _extract_candidates(html_text: str) -> set:
    soup = BeautifulSoup(html_text, "lxml")
    text = soup.get_text("\n", strip=True)
    cands = set()
    for line in text.splitlines():
        line = line.strip()
        if not line: continue
        if re.search(r"[–ê-–ØA-Z]", line) and len(line) > 3:
            for part in re.split(r"[;,‚Ä¢\u2022]", line):
                p = part.strip()
                if len(p) >= 3 and re.search(r"[–ê-–ØA-Z]", p):
                    cands.add(p)
    return cands

def _refresh_legal_if_needed(force: bool = False):
    if not LEGAL_ENABLED:
        return
    now = datetime.utcnow()
    if not force and _legal_cache["ts"] and now - _legal_cache["ts"] < timedelta(hours=LEGAL_REFRESH_HOURS):
        return
    try:
        agents = _extract_candidates(_http_get_text(URL_MINJUST))
    except Exception as e:
        log.warning("MINJUST fetch failed: %s", e)
        agents = _legal_cache.get("agents", set())
    try:
        banned = _extract_candidates(_http_get_text(URL_FSB))
    except Exception as e:
        log.warning("FSB fetch failed: %s", e)
        banned = _legal_cache.get("banned", set())

    alias_map = _build_alias_map(agents | banned)
    _legal_cache.update({"ts": now, "agents": set(agents), "banned": set(banned), "alias_map": alias_map})
    log.info("Compliance refreshed: agents=%d, banned=%d, aliases=%d",
             len(agents), len(banned), len(alias_map))

def _apply_star_once(text: str, token: str) -> str:
    # –¥–æ–±–∞–≤–ª—è–µ–º * —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ ¬´—á–∏—Å—Ç–æ–≥–æ¬ª –≤—Ö–æ–∂–¥–µ–Ω–∏—è
    pat = re.compile(rf"(?<!\*)\b{re.escape(token)}\b")
    m = pat.search(text)
    if m:
        i = m.end()
        if not (i < len(text) and text[i] == "*"):
            text = text[:i] + "*" + text[i:]
    return text

def apply_legal_marks(text: str) -> str:
    if not LEGAL_ENABLED: return text
    _refresh_legal_if_needed()

    notes = []
    out = text
    text_norm = _norm(text)

    # –Ω–∞–π–¥—ë–º –∞–ª–∏–∞—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
    found = {}
    for alias_norm, canonical in _legal_cache["alias_map"].items():
        if alias_norm and alias_norm in text_norm:
            typ = None
            if canonical in _legal_cache["agents"]:
                typ = "agent"
            if canonical in _legal_cache["banned"]:
                typ = "banned" if typ is None else typ
            if typ:
                found.setdefault(canonical, typ)

    # –ø—Ä–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–≤—ë–∑–¥–æ—á–∫–∏ –∏ –Ω–∞–±–∏—Ä–∞–µ–º —Ñ—É—Ç–Ω–æ—Ç—ã
    for canonical, typ in found.items():
        # –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º —Ç–æ–∫–µ–Ω–∞
        candidates = {canonical, canonical.capitalize()}
        for cand in sorted(candidates, key=len, reverse=True):
            out = _apply_star_once(out, cand)

        # —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ —Å–Ω–æ—Å–æ–∫ ‚Äî –∫–∞–∫ –¥–æ–≥–æ–≤–æ—Ä–µ–Ω–æ (–∫—É—Ä—Å–∏–≤)
        if typ == "banned":
            if _norm(canonical) == "meta":
                note = "_*Meta –ø—Ä–∏–∑–Ω–∞–Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∏—Å—Ç—Å–∫–æ–π –∏ –∑–∞–ø—Ä–µ—â–µ–Ω–∞ –≤ –†–§._"
            else:
                note = "_*–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–Ω–∞ —ç–∫—Å—Ç—Ä–µ–º–∏—Å—Ç—Å–∫–æ–π –∏ –∑–∞–ø—Ä–µ—â–µ–Ω–∞ –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–§._"
        else:
            note = "_*–ü—Ä–∏–∑–Ω–∞–Ω –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–æ—Å—Å–∏–∏_"
        if note not in notes:
            notes.append(note)

    if notes:
        out += "\n\n" + "\n".join(notes)
    return out

# ----------------- –ü–û–°–¢–ò–ù–ì -----------------
def send_text(text: str, image_url: Optional[str] = None):
    try:
        if image_url:
            bot.send_photo(CHANNEL_ID, image_url, caption=(text[:1024] if text else ""))
        else:
            bot.send_message(CHANNEL_ID, text[:4096])
    except Exception as e:
        log.error("Telegram send error: %s", e)
        raise

# ----------------- RSS POLLER -----------------
def poller():
    feeds = load_feeds()
    if not feeds:
        log.warning("FEEDS_JSON –ø—É—Å—Ç ‚Äî –Ω–µ—Ç –ª–µ–Ω—Ç –¥–ª—è –æ–ø—Ä–æ—Å–∞.")
        return
    log.info("RSS poller: %d –ª–µ–Ω—Ç, –∏–Ω—Ç–µ—Ä–≤–∞–ª %d —Å–µ–∫.", len(feeds), RSS_POLL_SEC)

    while True:
        try:
            for url in feeds:
                try:
                    d = feedparser.parse(url)
                    for e in d.entries[:40]:
                        link = getattr(e, "link", "") or ""
                        if not link:
                            continue
                        uid = getattr(e, "id", "") or link
                        if uid in seen_ids:
                            continue

                        title = (getattr(e, "title", "") or "").strip()
                        summary = strip_html(getattr(e, "summary", "") or getattr(e, "description", "") or "")
                        if not title and not summary:
                            continue

                        if not allowed_topic(title, summary):
                            continue

                        item = NewsItem(
                            title=title,
                            summary=summary,
                            link=link,
                            published=None,
                            breaking=is_breaking(title, summary),
                        )
                        seen_ids.add(uid)
                        news_q.put(item)
                except Exception as fe:
                    log.warning("Feed error %s: %s", url, fe)
        except Exception as e:
            log.exception("RSS poller error: %s", e)

        time.sleep(RSS_POLL_SEC)

# ----------------- –ü–£–ë–õ–ò–ö–ê–¢–û–† -----------------
def publisher():
    log.info("Publisher: –∏–Ω—Ç–µ—Ä–≤–∞–ª %d –º–∏–Ω, breaking=%s, mode=%s", POST_INTERVAL_MIN, ALLOW_BREAKING, POST_MODE)
    last_post_ts = 0.0
    interval = max(10, POST_INTERVAL_MIN * 60)

    while True:
        try:
            # –°—Ä–æ—á–Ω—ã–µ ‚Äî —Å—Ä–∞–∑—É
            if ALLOW_BREAKING:
                try:
                    it = news_q.get_nowait()
                    if it.breaking:
                        log.info("BREAKING: %s", it.title[:120])
                        publish_item(it)
                        last_post_ts = time.time()
                        continue
                    else:
                        news_q.put(it)
                except queue.Empty:
                    pass

            # –ü–ª–∞–Ω–æ–≤–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è
            if time.time() - last_post_ts >= interval:
                try:
                    it = news_q.get_nowait()
                except queue.Empty:
                    log.info("–û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞, –∂–¥—ë–º...")
                    time.sleep(3)
                    continue

                log.info("Scheduled post: %s", it.title[:120])
                publish_item(it)
                last_post_ts = time.time()

            time.sleep(2)
        except Exception as e:
            log.exception("Publisher error: %s", e)
            time.sleep(5)

def publish_item(item: NewsItem):
    emoji, rubric = pick_rubric(item.title, item.summary, item.link)
    text = gpt_generate_post(item.title, item.summary, item.link, emoji, rubric)
    # –ö–æ–º–ø–ª–∞–µ–Ω—Å –†–§
    text = apply_legal_marks(text)

    image_url = None
    if ALLOW_IMAGES:
        # feedparser enclosures
        # –º–Ω–æ–≥–∏–µ RSS –∫–ª–∞–¥—É—Ç –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ e.media_content / e.enclosures ‚Äî –º—ã –≤–∑—è–ª–∏ –∏–∑ summary,
        # –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –ø–µ—Ä–≤–æ–º –≤—Å—Ç—Ä–µ—á–Ω–æ–º img —Ç–µ–≥e
        m = re.search(r'(https?://\S+\.(?:jpg|jpeg|png|gif))', item.summary, re.IGNORECASE)
        if m:
            image_url = m.group(1)

    send_text(text, image_url)

# ----------------- MAIN -----------------
def main():
    me = bot.get_me()
    log.info("Telegram –±–æ—Ç: @%s", me.username)

    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–∏–º –∫–æ–º–ø–ª–∞–µ–Ω—Å-—Å–ø–∏—Å–∫–∏ –Ω–∞ —Å—Ç–∞—Ä—Ç–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
    _refresh_legal_if_needed(force=True)

    threading.Thread(target=poller, daemon=True).start()
    threading.Thread(target=publisher, daemon=True).start()

    while True:
        time.sleep(60)

if __name__ == "__main__":
    main()
